{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "57bnh5u5c3adru6segp2",
   "authorId": "5526226532556",
   "authorName": "DNAINTEGRASI",
   "authorEmail": "dna.dataintegrasi@gmail.com",
   "sessionId": "6e4acd48-a823-402f-9242-67109549f20b",
   "lastEditTime": 1761534476712
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abcba40-63e8-4419-ac99-6077eac4bb6e",
   "metadata": {
    "name": "title",
    "collapsed": false
   },
   "source": "# **TELCO CUSTOMER SEGMENTATION**\nTo better understand customer performance, non-churning customers are analyzed in greater detail to identify high- and low-performing segments.\n\n---"
  },
  {
   "cell_type": "code",
   "id": "d3326d18-43d5-432e-b22a-798cd2c42d63",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "libraries"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.compose import ColumnTransformer\n\n# encoding\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom category_encoders import OrdinalEncoder, BinaryEncoder\n\n# scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler \n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# export model\nimport pickle\nimport joblib\n\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "non_churning_customers"
   },
   "source": "WITH LIST_DATES AS (\n    SELECT \n        MAX(PREDICTION_DATE) AS dates\n    FROM TELCO.DATAMART.TB_F_CUSTOMER_SEGMENTATION\n)\nSELECT \n    -- a.PREDICTION_DATE\n    a.CUSTOMER_ID\n    ,b.GENDER\n    ,b.SENIOR_CITIZEN\n    ,b.PARTNER\n    ,b.DEPENDENTS\n    ,b.STATE\n    ,b.CITY\n    ,b.LATITUDE\n    ,b.LONGITUDE\n    ,f.QUARTER AS QUARTER_JOINED\n    ,c.PHONE_SERVICE\n    ,c.MULTIPLE_LINES\n    ,c.INTERNET_SERVICE\n    ,c.ONLINE_SECURITY\n    ,c.ONLINE_BACKUP\n    ,c.DEVICE_PROTECTION\n    ,c.TECH_SUPPORT\n    ,c.STREAMING_TV\n    ,c.STREAMING_MOVIES\n    ,c.CONTRACT_TYPE\n    ,c.PAPERLESS_BILLING\n    ,c.PAYMENT_METHOD\n    ,DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) AS TENURE_MONTHS\n    ,d.MONTHLY_CHARGES\n    ,DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) * d.MONTHLY_CHARGES AS TOTAL_CHARGES\n    ,CASE \n        WHEN DATEDIFF('YEAR', b.DATE_JOINED, a.PREDICTION_DATE) <= 1 THEN 1 * (DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) * d.MONTHLY_CHARGES)\n        ELSE DATEDIFF('YEAR', b.DATE_JOINED, a.PREDICTION_DATE) * (DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) * d.MONTHLY_CHARGES)\n    END AS CLTV \nFROM TELCO.DATAMART.TB_R_CHURN_PREDICTION a\nLEFT JOIN TELCO.DATAMART.TB_R_CUSTOMER b ON a.customer_id = b.customer_id\nLEFT JOIN TELCO.DATAMART.TB_F_SERVICE_USAGE c ON a.customer_id = c.customer_id\nLEFT JOIN TELCO.DATAMART.TB_F_REVENUE d ON a.customer_id = d.customer_id\nLEFT JOIN TELCO.DATAMART.TB_R_DATE f ON d.date_joined = f.date_id\nLEFT JOIN LIST_DATES g ON a.PREDICTION_DATE = g.dates\nWHERE a.prediction_results = 'No'\n    AND g.dates IS NULL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "data_cleansing"
   },
   "source": "df = non_churning_customers.to_pandas()\ndf_segmentation = df.copy()\n\ndf_segmentation = df_segmentation.drop(columns= 'CUSTOMER_ID')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "979b70e7-cb64-4a71-9858-1d19f8e66b5d",
   "metadata": {
    "language": "python",
    "name": "preprocessing"
   },
   "outputs": [],
   "source": "X = df_segmentation\n\ntransformer = ColumnTransformer([\n    ('onehot', OneHotEncoder(drop='first'), [\n        'GENDER', 'SENIOR_CITIZEN', 'PARTNER', 'DEPENDENTS', 'PHONE_SERVICE', 'MULTIPLE_LINES', 'INTERNET_SERVICE', 'ONLINE_SECURITY', 'ONLINE_BACKUP', 'DEVICE_PROTECTION', 'TECH_SUPPORT', 'STREAMING_TV', 'STREAMING_MOVIES', 'CONTRACT_TYPE', 'PAPERLESS_BILLING', 'PAYMENT_METHOD' \n    ]),\n    ('binary', BinaryEncoder(), [\n        'STATE', 'CITY', 'QUARTER_JOINED'\n    ]),\n    ('robust', RobustScaler(), [\n        'LATITUDE', 'LONGITUDE', 'TENURE_MONTHS', 'MONTHLY_CHARGES', 'TOTAL_CHARGES', 'CLTV'\n    ])\n], remainder='passthrough')\n\nX_scaled = transformer.fit_transform(X)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aea8874-0bd5-4cad-afe8-7a44266ab43c",
   "metadata": {
    "language": "python",
    "name": "best_n_component"
   },
   "outputs": [],
   "source": "# define PCA\npca = PCA()\n\n# fit\npca.fit(X_scaled)\n\n# transform\nX_pca = pca.transform(X_scaled)\n\n# PCA variance \ndf_pca = pd.DataFrame()\ndf_pca['PC'] = range(1,43,1)\ndf_pca['variance'] = pca.explained_variance_ratio_.round(3)\ndf_pca['cumsum_variance'] = pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize= (15,5))\n\n# membuat grid\nsns.set_style('whitegrid')\n\nsns.lineplot(data= df_pca, x= 'PC', y= 'cumsum_variance')\nsns.scatterplot(data= df_pca, x= 'PC', y= 'cumsum_variance')\n\nplt.xticks(df_pca['PC'])\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d182e378-bc9a-4058-a5bd-ea3f118593d2",
   "metadata": {
    "language": "python",
    "name": "top_3_pca_feature"
   },
   "outputs": [],
   "source": "# --- 1️⃣ Fit your transformer ---\ntransformer.fit(X)\n\n# --- 2️⃣ Helper: extract transformed feature names ---\ndef get_feature_names(column_transformer):\n    output_features = []\n    for name, trans, cols in column_transformer.transformers_:\n        if name == 'remainder':\n            continue\n        if hasattr(trans, 'get_feature_names_out'):\n            names = trans.get_feature_names_out(cols)\n        else:\n            names = cols\n        output_features.extend(names)\n    return output_features\n\nX_transformed_cols = get_feature_names(transformer)\n\n# --- 3️⃣ Apply PCA ---\npca = PCA()\nX_pca = pca.fit_transform(transformer.transform(X))\n\n# --- 4️⃣ Find top 3 features per component ---\nlist_imp = []\nfor i in range(pca.components_.shape[0]):\n    pc_i_importance = pd.DataFrame({\n        'feature': X_transformed_cols,\n        'coef': np.abs(pca.components_[i])\n    }).sort_values(by='coef', ascending=False).head(3)\n    \n    # Store top 3 features for that PC\n    list_imp.append(list(pc_i_importance['feature'].values))\n\n# --- 5️⃣ Build PCA summary dataframe ---\ndf_pca = pd.DataFrame({\n    'PC': range(1, pca.n_components_ + 1),\n    'variance': pca.explained_variance_ratio_.round(3),\n    'cumsum_variance': pca.explained_variance_ratio_.cumsum()\n})\n\n# Ensure feature importance columns are cleanly added\ndf_pca = df_pca.drop(columns=['feature_importance', 'Top1', 'Top2', 'Top3'], errors='ignore')\n\ntop_features = pd.DataFrame(list_imp, columns=['Top1', 'Top2', 'Top3'])\ndf_pca = pd.concat([df_pca, top_features], axis=1)\n\n# --- 6️⃣ Display result ---\ndf_pca.head(15)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5d8700c-576f-495b-9b90-11c76e8213c4",
   "metadata": {
    "language": "python",
    "name": "best_component"
   },
   "outputs": [],
   "source": "pca_11com = PCA(n_components= 11)\n\n# fit and transform\nX_pca_11com = pca_11com.fit_transform(X_scaled)\n\ndf_11com = pd.DataFrame(data= X_pca_11com, columns= ['PC0', 'PC1','PC2', 'PC3','PC4', 'PC5','PC6', 'PC7','PC8', 'PC9', 'PC10'])\ndf_11com.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86dceaa2-a114-4bb0-8ca7-644ec1130b02",
   "metadata": {
    "language": "python",
    "name": "best_n_cluster"
   },
   "outputs": [],
   "source": "list_n_cluster = range(2, 10)\nlist_inertia = []\n\nfor i in list_n_cluster:\n    # define model\n    kmeans = KMeans(n_clusters=i, random_state=0)\n\n    # fit on PCA data\n    kmeans.fit(df_11com)\n\n    # inertia (within-cluster sum of squares)\n    inertia = kmeans.inertia_\n    list_inertia.append(inertia)\n\n# visualize elbow and silhouette\nplt.figure(figsize=(10,5))\nplt.plot(list_n_cluster, list_inertia, 'o-', label='Inertia (Elbow)')\nplt.tight_layout()\nplt.legend()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03d2ac80-586b-4495-9d67-687d1a50d52f",
   "metadata": {
    "language": "python",
    "name": "best_model"
   },
   "outputs": [],
   "source": "kmeans_best = KMeans(n_clusters= 4, random_state= 0)\n\nkmeans_best.fit(X_scaled)\n\ndf_clustered = pd.DataFrame()\ndf_clustered['CUSTOMER_ID'] = df['CUSTOMER_ID']\ndf_clustered['CUSTOMER_CLUSTER'] = kmeans_best.labels_\n\ndf_clustered.head(10)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0208aa44-4f70-40b1-b4e4-33d1cbaf5233",
   "metadata": {
    "language": "sql",
    "name": "segmentation"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE TELCO.DATAMART.SP_CUSTOMER_SEGMENTATION()\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.9'\nPACKAGES = ('snowflake-snowpark-python', 'pandas', 'scikit-learn', 'category_encoders')\nHANDLER = 'main'\nAS\n$$\nimport pandas as pd\nfrom snowflake.snowpark.functions import col\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\nfrom category_encoders.binary import BinaryEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\ndef main(session) -> str:\n\n    # ================\n    # 1️⃣ Load Data\n    # ================\n    query = \"\"\"\n        WITH LIST_DATES AS (\n            SELECT \n                MAX(PREDICTION_DATE) AS dates\n            FROM TELCO.DATAMART.TB_F_CUSTOMER_CLUSTER\n        )\n        SELECT \n            a.CUSTOMER_ID,\n            b.GENDER,\n            b.SENIOR_CITIZEN,\n            b.PARTNER,\n            b.DEPENDENTS,\n            b.STATE,\n            b.CITY,\n            b.LATITUDE,\n            b.LONGITUDE,\n            f.QUARTER AS QUARTER_JOINED,\n            c.PHONE_SERVICE,\n            c.MULTIPLE_LINES,\n            c.INTERNET_SERVICE,\n            c.ONLINE_SECURITY,\n            c.ONLINE_BACKUP,\n            c.DEVICE_PROTECTION,\n            c.TECH_SUPPORT,\n            c.STREAMING_TV,\n            c.STREAMING_MOVIES,\n            c.CONTRACT_TYPE,\n            c.PAPERLESS_BILLING,\n            c.PAYMENT_METHOD,\n            DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) AS TENURE_MONTHS,\n            d.MONTHLY_CHARGES,\n            DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) * d.MONTHLY_CHARGES AS TOTAL_CHARGES,\n            CASE \n                WHEN DATEDIFF('YEAR', b.DATE_JOINED, a.PREDICTION_DATE) <= 1 THEN \n                    1 * (DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) * d.MONTHLY_CHARGES)\n                ELSE \n                    DATEDIFF('YEAR', b.DATE_JOINED, a.PREDICTION_DATE) * \n                    (DATEDIFF('MONTH', b.DATE_JOINED, a.PREDICTION_DATE) * d.MONTHLY_CHARGES)\n            END AS CLTV \n        FROM TELCO.DATAMART.TB_R_CHURN_PREDICTION a\n        LEFT JOIN TELCO.DATAMART.TB_R_CUSTOMER b ON a.customer_id = b.customer_id\n        LEFT JOIN TELCO.DATAMART.TB_F_SERVICE_USAGE c ON a.customer_id = c.customer_id\n        LEFT JOIN TELCO.DATAMART.TB_F_REVENUE d ON a.customer_id = d.customer_id\n        LEFT JOIN TELCO.DATAMART.TB_R_DATE f ON d.date_joined = f.date_id\n        LEFT JOIN LIST_DATES g ON a.PREDICTION_DATE = g.dates\n        WHERE a.prediction_results = 'No'\n          AND g.dates IS NULL\n    \"\"\"\n    df = session.sql(query).to_pandas()\n\n    # ================\n    # 2️⃣ Preprocess\n    # ================\n    df_seg = df.copy()\n    customer_ids = df_seg['CUSTOMER_ID']\n    df_seg = df_seg.drop(columns='CUSTOMER_ID')\n\n    transformer = ColumnTransformer([\n        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'), [\n            'GENDER', 'SENIOR_CITIZEN', 'PARTNER', 'DEPENDENTS',\n            'PHONE_SERVICE', 'MULTIPLE_LINES', 'INTERNET_SERVICE',\n            'ONLINE_SECURITY', 'ONLINE_BACKUP', 'DEVICE_PROTECTION',\n            'TECH_SUPPORT', 'STREAMING_TV', 'STREAMING_MOVIES',\n            'CONTRACT_TYPE', 'PAPERLESS_BILLING', 'PAYMENT_METHOD'\n        ]),\n        ('binary', BinaryEncoder(), ['STATE', 'CITY', 'QUARTER_JOINED']),\n        ('robust', RobustScaler(), [\n            'LATITUDE', 'LONGITUDE', 'TENURE_MONTHS',\n            'MONTHLY_CHARGES', 'TOTAL_CHARGES', 'CLTV'\n        ])\n    ], remainder='passthrough')\n\n    X_scaled = transformer.fit_transform(df_seg)\n\n    # ================\n    # 3️⃣ PCA (11 Components)\n    # ================\n    pca = PCA(n_components=11)\n    X_pca = pca.fit_transform(X_scaled)\n\n    # ================\n    # 4️⃣ K-Means (4 Clusters)\n    # ================\n    kmeans = KMeans(n_clusters=4, random_state=0)\n    clusters = kmeans.fit_predict(X_scaled)\n\n    df_result = pd.DataFrame({\n        'CUSTOMER_ID': customer_ids,\n        'CUSTOMER_CLUSTER': clusters\n    })\n\n    # Add prediction date from Snowflake\n    df_result[\"PREDICTION_DATE\"] = session.sql(\"SELECT CURRENT_DATE()\").collect()[0][0]\n    \n    # (Optional) Add a readable status or label mapping\n    df_result[\"PREDICTED_STATUS\"] = df_result[\"CUSTOMER_CLUSTER\"].replace({\n        0: \"Segment 1\",\n        1: \"Segment 2\",\n        2: \"Segment 3\",\n        3: \"Segment 4\"\n    })\n    df_export = pd.DataFrame()\n    df_export['PREDICTION_DATE'] = df_result['PREDICTION_DATE']\n    df_export['CUSTOMER_ID'] = df_result['CUSTOMER_ID']\n    df_export['CUSTOMER_SEGMENTATION'] = df_result['PREDICTED_STATUS']\n\n    # ================\n    # 5️⃣ Save Result Back to Snowflake\n    # ================\n    session.create_dataframe(df_export).write.mode(\"append\").save_as_table(\"TELCO.DATAMART.TB_F_CUSTOMER_CLUSTER\")\n\n    return f\"✅ Customer segmentation completed successfully! Total customers clustered: {len(df_export)}\"\n$$;",
   "execution_count": null
  }
 ]
}